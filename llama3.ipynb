{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN58ptuKBUlB",
        "outputId": "f6093a7f-50eb-40dd-ad2e-9a7464af3461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (0.2.0)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (0.7.0)\n",
            "Requirement already satisfied: torch in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (2.3.1)\n",
            "Requirement already satisfied: blobfile in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (2.1.1)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (3.8.4)\n",
            "Requirement already satisfied: huggingface_hub in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (0.23.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.6.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: pycryptodomex~=3.8 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from blobfile) (3.20.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from blobfile) (2.2.1)\n",
            "Requirement already satisfied: lxml~=4.9 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from blobfile) (4.9.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.52.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.21 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\sharanchi.m\\appdata\\roaming\\python\\python312\\site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sentencepiece tiktoken torch blobfile matplotlib huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d83c5fe0e28a4f4399471d9920e75672",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "from tiktoken.load import load_tiktoken_bpe\n",
        "\n",
        "import torch\n",
        "\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "tokenizer_model = load_tiktoken_bpe(\"tokenizer.model\")\n",
        "\n",
        "len(tokenizer_model)\n",
        "\n",
        "type(tokenizer_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{b'mitted': 5600,\n",
              " b\" $('#\": 5601,\n",
              " b' saw': 5602,\n",
              " b' approach': 5603,\n",
              " b'ICE': 5604,\n",
              " b' saying': 5605,\n",
              " b' anyone': 5606,\n",
              " b'meta': 5607,\n",
              " b'SD': 5608,\n",
              " b' song': 5609}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "dict(list(tokenizer_model.items())[5600:5610])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tok_embeddings.weight',\n",
              " 'layers.0.attention.wq.weight',\n",
              " 'layers.0.attention.wk.weight',\n",
              " 'layers.0.attention.wv.weight',\n",
              " 'layers.0.attention.wo.weight',\n",
              " 'layers.0.feed_forward.w1.weight',\n",
              " 'layers.0.feed_forward.w3.weight',\n",
              " 'layers.0.feed_forward.w2.weight',\n",
              " 'layers.0.attention_norm.weight',\n",
              " 'layers.0.ffn_norm.weight',\n",
              " 'layers.1.attention.wq.weight']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model = torch.load(\"consolidated.00.pth\")\n",
        "\n",
        "list(model.keys())[:11]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dim': 4096, 'n_layers': 32, 'n_heads': 32, 'n_kv_heads': 8, 'vocab_size': 128256, 'multiple_of': 1024, 'ffn_dim_multiplier': 1.3, 'norm_eps': 1e-05, 'rope_theta': 500000.0}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with open(\"original_params.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "dim = config[\"dim\"]\n",
        "\n",
        "n_layers = config[\"n_layers\"]\n",
        "\n",
        "n_heads = config[\"n_heads\"]\n",
        "\n",
        "n_kv_heads = config[\"n_kv_heads\"]\n",
        "\n",
        "vocab_size = config[\"vocab_size\"]\n",
        "\n",
        "multiple_of = config[\"multiple_of\"]\n",
        "\n",
        "ffn_dim_multiplier = config[\"ffn_dim_multiplier\"]\n",
        "\n",
        "norm_eps = config[\"norm_eps\"]\n",
        "\n",
        "rope_theta = torch.tensor(config[\"rope_theta\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tokenizing I/P data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "special_tokens = [\n",
        "    \"<|begin_of_text|>\",  # Marks the beginning of a text sequence.\n",
        "    \"<|end_of_text|>\",  # Marks the end of a text sequence.\n",
        "    \"<|reserved_special_token_0|>\",  # Reserved for future use.\n",
        "    \"<|reserved_special_token_1|>\",  # Reserved for future use.\n",
        "    \"<|reserved_special_token_2|>\",  # Reserved for future use.\n",
        "    \"<|reserved_special_token_3|>\",  # Reserved for future use.\n",
        "    \"<|start_header_id|>\",  # Indicates the start of a header ID.\n",
        "    \"<|end_header_id|>\",  # Indicates the end of a header ID.\n",
        "    \"<|reserved_special_token_4|>\",  # Reserved for future use.\n",
        "    \"<|eot_id|>\",  # Marks the end of a turn (in a conversational context).\n",
        "] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)]  # A large set of tokens reserved for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "tokenize_breaker = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello world!'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "tokenizer = tiktoken.Encoding(\n",
        "\n",
        "    name = \"tokenizer.model\",\n",
        "\n",
        "    pat_str = tokenize_breaker,\n",
        "\n",
        "    mergeable_ranks = tokenizer_model,\n",
        "\n",
        "    special_tokens={token: len(tokenizer_model) + i for i, token in enumerate(special_tokens)},\n",
        ")\n",
        "\n",
        "tokenizer.decode(tokenizer.encode(\"hello world!\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]\n",
            "['<|begin_of_text|>', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = \"the answer to the ultimate question of life, the universe, and everything is \"\n",
        "\n",
        "tokens = [128000] + tokenizer.encode(prompt)\n",
        "\n",
        "print(tokens)  \n",
        "\n",
        "tokens = torch.tensor(tokens)\n",
        "\n",
        "prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens]\n",
        "\n",
        "print(prompt_split_as_tokens) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings for each token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4096\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 4096])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, dim)\n",
        "\n",
        "embedding_layer.weight.data.copy_(model[\"tok_embeddings.weight\"])\n",
        "\n",
        "token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)\n",
        "\n",
        "token_embeddings_unnormalized.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalization using RMSNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def rms_norm(tensor, norm_weights):\n",
        "\n",
        "    squared_mean = tensor.pow(2).mean(-1, keepdim=True)\n",
        "\n",
        "    normalized = torch.rsqrt(squared_mean + norm_eps)\n",
        "\n",
        "    return (tensor * normalized) * norm_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 4096])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "token_embeddings = rms_norm(token_embeddings_unnormalized, \n",
        "                            model[\"layers.0.attention_norm.weight\"])\n",
        "\n",
        "token_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Attention Heads (Query,Key,Values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\n",
        "\n",
        "    model[\"layers.0.attention.wq.weight\"].shape,\n",
        "\n",
        "    model[\"layers.0.attention.wk.weight\"].shape,\n",
        "\n",
        "    model[\"layers.0.attention.wv.weight\"].shape,\n",
        "\n",
        "    model[\"layers.0.attention.wo.weight\"].shape\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 128, 4096])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "q_layer0 = model[\"layers.0.attention.wq.weight\"]\n",
        "\n",
        "head_dim = q_layer0.shape[0] // n_heads\n",
        "\n",
        "q_layer0 = q_layer0.view(n_heads, head_dim, dim)\n",
        "\n",
        "q_layer0.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([128, 4096])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "q_layer0_head0 = q_layer0[0]\n",
        "\n",
        "q_layer0_head0.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 128])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)\n",
        "\n",
        "q_per_token.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementing RoPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 64, 2])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
        "\n",
        "q_per_token_split_into_pairs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,\n",
              "        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,\n",
              "        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,\n",
              "        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,\n",
              "        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,\n",
              "        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,\n",
              "        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,\n",
              "        0.9844])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "zero_to_one_split_into_64_parts = torch.tensor(range(64))/64\n",
        "\n",
        "zero_to_one_split_into_64_parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,\n",
              "        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,\n",
              "        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,\n",
              "        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,\n",
              "        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,\n",
              "        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,\n",
              "        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,\n",
              "        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,\n",
              "        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,\n",
              "        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,\n",
              "        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)\n",
        "\n",
        "freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 64])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
        "\n",
        "q_per_token_as_complex_numbers.shape\n",
        "\n",
        "freqs_for_each_token = torch.outer(torch.arange(17), freqs)\n",
        "\n",
        "freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)\n",
        "\n",
        "q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis\n",
        "\n",
        "q_per_token_as_complex_numbers_rotated.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 64, 2])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)\n",
        "\n",
        "q_per_token_split_into_pairs_rotated.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 128])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)\n",
        "\n",
        "q_per_token_rotated.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 128])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "k_layer0 = model[\"layers.0.attention.wk.weight\"]\n",
        "\n",
        "k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[0] // n_kv_heads, dim)\n",
        "\n",
        "k_layer0.shape \n",
        "\n",
        "k_layer0_head0 = k_layer0[0]\n",
        "\n",
        "k_layer0_head0.shape \n",
        "\n",
        "k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T)\n",
        "\n",
        "k_per_token.shape \n",
        "\n",
        "k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)\n",
        "\n",
        "k_per_token_split_into_pairs.shape  \n",
        "\n",
        "k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)\n",
        "\n",
        "k_per_token_as_complex_numbers.shape \n",
        "\n",
        "k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)\n",
        "\n",
        "k_per_token_split_into_pairs_rotated.shape \n",
        "\n",
        "k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)\n",
        "\n",
        "k_per_token_rotated.shape "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementation of self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 17])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T) / (head_dim) ** 0.5\n",
        "\n",
        "qk_per_token.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "mask = torch.full((len(tokens), len(tokens)), float(\"-inf\"), device=tokens.device)\n",
        "\n",
        "mask = torch.triu(mask, diagonal=1)\n",
        "\n",
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "qk_per_token_after_masking = qk_per_token + mask\n",
        "\n",
        "qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 128, 4096])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "v_layer0 = model[\"layers.0.attention.wv.weight\"]\n",
        "\n",
        "v_layer0 = v_layer0.view(n_kv_heads, v_layer0.shape[0] // n_kv_heads, dim)\n",
        "\n",
        "v_layer0.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([128, 4096])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "v_layer0_head0 = v_layer0[0]\n",
        "\n",
        "v_layer0_head0.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 128])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "v_per_token = torch.matmul(token_embeddings, v_layer0_head0.T)\n",
        "\n",
        "v_per_token.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 128])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
        "\n",
        "qkv_attention.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementing multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "qkv_attention_store = []\n",
        "\n",
        "for head in range(n_heads):\n",
        "\n",
        "    q_layer0_head = q_layer0[head]\n",
        "    k_layer0_head = k_layer0[head//4]  \n",
        "    v_layer0_head = v_layer0[head//4] \n",
        "\n",
        "    q_per_token = torch.matmul(token_embeddings, q_layer0_head.T)\n",
        "\n",
        "    k_per_token = torch.matmul(token_embeddings, k_layer0_head.T)\n",
        "\n",
        "    v_per_token = torch.matmul(token_embeddings, v_layer0_head.T)\n",
        "\n",
        "    q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
        "    q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
        "    q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis[:len(tokens)])\n",
        "    q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)\n",
        "\n",
        "    k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)\n",
        "    k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)\n",
        "    k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis[:len(tokens)])\n",
        "    k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)\n",
        "\n",
        "    qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T) / (128) ** 0.5\n",
        "\n",
        "    mask = torch.full((len(tokens), len(tokens)), float(\"-inf\"), device=tokens.device)\n",
        "\n",
        "    mask = torch.triu(mask, diagonal=1)\n",
        "\n",
        "    qk_per_token_after_masking = qk_per_token + mask\n",
        "\n",
        "    qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)\n",
        "\n",
        "    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
        "\n",
        "    qkv_attention_store.append(qkv_attention)\n",
        "\n",
        "len(qkv_attention_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 4096])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)\n",
        "\n",
        "stacked_qkv_attention.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 4096])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "embedding_delta = torch.matmul(stacked_qkv_attention, model[\"layers.0.attention.wo.weight\"].T)\n",
        "\n",
        "embedding_delta.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 4096])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "embedding_after_edit = token_embeddings_unnormalized + embedding_delta\n",
        "\n",
        "embedding_after_edit.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 4096])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[\"layers.0.ffn_norm.weight\"])\n",
        "\n",
        "embedding_after_edit_normalized.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementing Swiglu activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 4096])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "w1 = model[\"layers.0.feed_forward.w1.weight\"]\n",
        "w2 = model[\"layers.0.feed_forward.w2.weight\"]\n",
        "w3 = model[\"layers.0.feed_forward.w3.weight\"]\n",
        "\n",
        "output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)\n",
        "\n",
        "output_after_feedforward.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merging to all the 31 more layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "final_embedding = token_embeddings_unnormalized\n",
        "\n",
        "for layer in range(n_layers):\n",
        "\n",
        "    qkv_attention_store = []\n",
        "\n",
        "    layer_embedding_norm = rms_norm(final_embedding, model[f\"layers.{layer}.attention_norm.weight\"])\n",
        "\n",
        "    q_layer = model[f\"layers.{layer}.attention.wq.weight\"]\n",
        "    q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim)\n",
        "    k_layer = model[f\"layers.{layer}.attention.wk.weight\"]\n",
        "    k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)\n",
        "    v_layer = model[f\"layers.{layer}.attention.wv.weight\"]\n",
        "    v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)\n",
        "    w_layer = model[f\"layers.{layer}.attention.wo.weight\"]\n",
        "\n",
        "    for head in range(n_heads):\n",
        "\n",
        "        q_layer_head = q_layer[head]\n",
        "        k_layer_head = k_layer[head//4] \n",
        "        v_layer_head = v_layer[head//4] \n",
        "\n",
        "        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)\n",
        "\n",
        "        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)\n",
        "  \n",
        "        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)\n",
        "\n",
        "        q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
        "        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
        "        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis)\n",
        "        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)\n",
        "\n",
        "        k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)\n",
        "        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)\n",
        "        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)\n",
        "        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)\n",
        "\n",
        "        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T) / (128) ** 0.5\n",
        " \n",
        "        mask = torch.full((len(token_embeddings_unnormalized), len(token_embeddings_unnormalized)), float(\"-inf\"))\n",
        "      \n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "\n",
        "        qk_per_token_after_masking = qk_per_token + mask\n",
        "        \n",
        "        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)\n",
        "\n",
        "        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
        "\n",
        "        qkv_attention_store.append(qkv_attention)\n",
        "\n",
        "    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)\n",
        " \n",
        "    embedding_delta = torch.matmul(stacked_qkv_attention, w_layer.T)\n",
        "\n",
        "    embedding_after_edit = final_embedding + embedding_delta\n",
        "\n",
        "    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f\"layers.{layer}.ffn_norm.weight\"])\n",
        "\n",
        "    w1 = model[f\"layers.{layer}.feed_forward.w1.weight\"]\n",
        "    w2 = model[f\"layers.{layer}.feed_forward.w2.weight\"]\n",
        "    w3 = model[f\"layers.{layer}.feed_forward.w3.weight\"]\n",
        "\n",
        "    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)\n",
        "\n",
        "    final_embedding = embedding_after_edit + output_after_feedforward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generating o/p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17, 4096])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "final_embedding = rms_norm(final_embedding, model[\"norm.weight\"])\n",
        "\n",
        "final_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([128256, 4096])"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model[\"output.weight\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([128256])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "logits = torch.matmul(final_embedding[-1], model[\"output.weight\"].T)\n",
        "\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2983)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "next_token = torch.argmax(logits, dim=-1)\n",
        "\n",
        "next_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'42'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "tokenizer.decode([next_token.item()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can experiment with different input texts by simply changing these two lines throughout the entire code..!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "prompt = \"Your Input\"\n",
        "\n",
        "freqs_for_each_token = torch.outer(torch.arange(17), freqs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
